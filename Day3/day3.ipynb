{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e758cc1",
   "metadata": {},
   "source": [
    "# 1.Data cleaning\n",
    "### Problem 1: Handle Missing Values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c8ee75",
   "metadata": {},
   "source": [
    "a) Create a dataset (python dictionary) with a tabular structure (like an Excelsheet) with columns Name, Age, and Salary. Create 10 rows.\n",
    "Example: Name: 'Alice', 'Bob', NAN, 'David', NAN,\n",
    "Age: 25, NAN, 28, 35, 22,\n",
    "Salary: 50000, 54000, NAN, 58000, 60000\n",
    "\n",
    "\n",
    "b) Convert the dictionary into dataframe.\n",
    "\n",
    "\n",
    "c) Fill missing 'Age' with mean\n",
    "\n",
    "\n",
    "d) Drop rows with missing 'Name'\n",
    "\n",
    "\n",
    "e) Fill missing 'Salary' with forward fill\n",
    "\n",
    "\n",
    "f) Print the Cleaned Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d8d0de6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original DataFrame:\n",
      "      Name   Age   Salary\n",
      "0   Alice  25.0  50000.0\n",
      "1     Bob   NaN  54000.0\n",
      "2     NaN  28.0      NaN\n",
      "3   David  35.0  58000.0\n",
      "4     NaN  22.0  60000.0\n",
      "5     Eva  30.0      NaN\n",
      "6   Frank   NaN  62000.0\n",
      "7     NaN  26.0  64000.0\n",
      "8  Hannah  24.0      NaN\n",
      "9     Ian   NaN  66000.0 \n",
      "\n",
      "Cleaned DataFrame:\n",
      "      Name        Age   Salary\n",
      "0   Alice  25.000000  50000.0\n",
      "1     Bob  27.142857  54000.0\n",
      "3   David  35.000000  58000.0\n",
      "5     Eva  30.000000  58000.0\n",
      "6   Frank  27.142857  62000.0\n",
      "8  Hannah  24.000000  62000.0\n",
      "9     Ian  27.142857  66000.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rohit\\AppData\\Local\\Temp\\ipykernel_31592\\1849069157.py:17: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['Age'].fillna(df['Age'].mean(), inplace=True)\n",
      "C:\\Users\\rohit\\AppData\\Local\\Temp\\ipykernel_31592\\1849069157.py:23: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['Salary'].fillna(method='ffill', inplace=True)\n",
      "C:\\Users\\rohit\\AppData\\Local\\Temp\\ipykernel_31592\\1849069157.py:23: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df['Salary'].fillna(method='ffill', inplace=True)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# a) Create the dataset with missing values\n",
    "data = {\n",
    "    'Name': ['Alice', 'Bob', np.nan, 'David', np.nan, 'Eva', 'Frank', np.nan, 'Hannah', 'Ian'],\n",
    "    'Age': [25, np.nan, 28, 35, 22, 30, np.nan, 26, 24, np.nan],\n",
    "    'Salary': [50000, 54000, np.nan, 58000, 60000, np.nan, 62000, 64000, np.nan, 66000]\n",
    "}\n",
    "\n",
    "# b) Convert the dictionary into a DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "print(\"Original DataFrame:\\n\", df, \"\\n\")\n",
    "\n",
    "# c) Fill missing 'Age' with the mean\n",
    "df['Age'].fillna(df['Age'].mean(), inplace=True)\n",
    "\n",
    "# d) Drop rows with missing 'Name'\n",
    "df.dropna(subset=['Name'], inplace=True)\n",
    "\n",
    "# e) Fill missing 'Salary' using forward fill\n",
    "df['Salary'].fillna(method='ffill', inplace=True)\n",
    "\n",
    "# f) Print the Cleaned Data\n",
    "print(\"Cleaned DataFrame:\\n\", df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d93151",
   "metadata": {},
   "source": [
    "# Problem 2: Remove Duplicates\n",
    "a) Create a dataset (python dictionary) with a tabular structure (like an Excelsheet) with columns ID and Score. Create 10 rows with duplicate records.\n",
    "\n",
    "\n",
    "b) Convert the dictionary into dataframe.\n",
    "\n",
    "\n",
    "c) Remove duplicates based on 'ID'\n",
    "\n",
    "\n",
    "d) Print the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1987b836",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original DataFrame with duplicates:\n",
      "     ID  Score\n",
      "0  101     90\n",
      "1  102     85\n",
      "2  103     88\n",
      "3  104     92\n",
      "4  101     90\n",
      "5  102     85\n",
      "6  105     80\n",
      "7  106     75\n",
      "8  107     70\n",
      "9  105     80 \n",
      "\n",
      "DataFrame after removing duplicates:\n",
      "     ID  Score\n",
      "0  101     90\n",
      "1  102     85\n",
      "2  103     88\n",
      "3  104     92\n",
      "6  105     80\n",
      "7  106     75\n",
      "8  107     70\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# a) Create a dataset with duplicates\n",
    "data = {\n",
    "    'ID': [101, 102, 103, 104, 101, 102, 105, 106, 107, 105],\n",
    "    'Score': [90, 85, 88, 92, 90, 85, 80, 75, 70, 80]\n",
    "}\n",
    "\n",
    "# b) Convert the dictionary into a DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "print(\"Original DataFrame with duplicates:\\n\", df, \"\\n\")\n",
    "\n",
    "# c) Remove duplicates based on 'ID' (keep first occurrence)\n",
    "df_unique = df.drop_duplicates(subset='ID', keep='first')\n",
    "\n",
    "# d) Print the DataFrame without duplicates\n",
    "print(\"DataFrame after removing duplicates:\\n\", df_unique)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a96df755",
   "metadata": {},
   "source": [
    "# 2. Data Transformation\n",
    "Problem 1: Normalize a Column (MinMaxScaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ddcf8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized Column:\n",
      "    Score  Normalized_Score\n",
      "0     45               0.2\n",
      "1     67               0.6\n",
      "2     89               1.0\n",
      "3     34               0.0\n",
      "4     56               0.4\n",
      "5     78               0.8\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "df1 = pd.DataFrame({\n",
    "    'Score': [45, 67, 89, 34, 56, 78]\n",
    "})\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "df1['Normalized_Score'] = scaler.fit_transform(df1[['Score']])\n",
    "\n",
    "print(\"Normalized Column:\\n\", df1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd68730",
   "metadata": {},
   "source": [
    "Problem 2: Convert Categorical to Numeric (One-hot Encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bde45594",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One-hot Encoded Data:\n",
      "       Name  City_Chennai  City_Delhi  City_Mumbai\n",
      "0    Alice         False        True        False\n",
      "1      Bob         False       False         True\n",
      "2  Charlie         False        True        False\n",
      "3    Alice          True       False        False\n"
     ]
    }
   ],
   "source": [
    "# Sample Data\n",
    "df2 = pd.DataFrame({\n",
    "    'Name': ['Alice', 'Bob', 'Charlie', 'Alice'],\n",
    "    'City': ['Delhi', 'Mumbai', 'Delhi', 'Chennai']\n",
    "})\n",
    "\n",
    "df_encoded = pd.get_dummies(df2, columns=['City'])\n",
    "\n",
    "print(\"One-hot Encoded Data:\\n\", df_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "130f9195",
   "metadata": {},
   "source": [
    " Problem 3: Log Transformation to Reduce Skew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1c8560",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log Transformed Data:\n",
      "    Income  Log_Income\n",
      "0    1000    6.907755\n",
      "1    2000    7.600902\n",
      "2    3000    8.006368\n",
      "3   10000    9.210340\n",
      "4   50000   10.819778\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "df3 = pd.DataFrame({\n",
    "    'Income': [1000, 2000, 3000, 10000, 50000]\n",
    "})\n",
    "\n",
    "df3['Log_Income'] = np.log(df3['Income'])\n",
    "\n",
    "print(\"Log Transformed Data:\\n\", df3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b08e8631",
   "metadata": {},
   "source": [
    "# 3. Data Reduction\n",
    "Problem 1: Feature Selection (Variance Threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "13787b23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Feature Selection:\n",
      "    Feature2\n",
      "0        10\n",
      "1        20\n",
      "2        15\n",
      "3        25\n",
      "4        30\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "df4 = pd.DataFrame({\n",
    "    'Feature1': [1, 1, 1, 1, 1],\n",
    "    'Feature2': [10, 20, 15, 25, 30],\n",
    "    'Feature3': [2, 2, 2, 2, 2],\n",
    "})\n",
    "\n",
    "selector = VarianceThreshold(threshold=0.0)\n",
    "reduced = selector.fit_transform(df4)\n",
    "\n",
    "df_reduced = pd.DataFrame(reduced, columns=df4.columns[selector.get_support()])\n",
    "print(\"After Feature Selection:\\n\", df_reduced)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df749b33",
   "metadata": {},
   "source": [
    " Problem 2: Dimensionality Reduction with PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "320f0536",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA Result:\n",
      "         PC1           PC2\n",
      "0  2.449490  7.063777e-17\n",
      "1  1.224745 -2.354592e-17\n",
      "2 -0.000000 -0.000000e+00\n",
      "3 -1.224745  2.354592e-17\n",
      "4 -2.449490  4.709185e-17\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "df5 = pd.DataFrame({\n",
    "    'X1': [1, 2, 3, 4, 5],\n",
    "    'X2': [2, 4, 6, 8, 10],\n",
    "    'X3': [5, 4, 3, 2, 1]\n",
    "})\n",
    "\n",
    "scaler = StandardScaler()\n",
    "df_scaled = scaler.fit_transform(df5)\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "df_pca = pca.fit_transform(df_scaled)\n",
    "\n",
    "print(\"PCA Result:\\n\", pd.DataFrame(df_pca, columns=['PC1', 'PC2']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c26f14f2",
   "metadata": {},
   "source": [
    "Problem 3: Aggregation (Reducing Rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a9b37f07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregated Data:\n",
      "              Salary\n",
      "Department         \n",
      "HR          42500.0\n",
      "IT          61000.0\n",
      "Sales       51000.0\n"
     ]
    }
   ],
   "source": [
    "df6 = pd.DataFrame({\n",
    "    'Department': ['HR', 'HR', 'IT', 'IT', 'Sales', 'Sales'],\n",
    "    'Salary': [40000, 45000, 60000, 62000, 50000, 52000]\n",
    "})\n",
    "\n",
    "df_agg = df6.groupby('Department').mean(numeric_only=True)\n",
    "\n",
    "print(\"Aggregated Data:\\n\", df_agg)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
